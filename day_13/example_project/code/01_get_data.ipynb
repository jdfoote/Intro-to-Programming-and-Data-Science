{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Project\n",
    "\n",
    "## Structure overview\n",
    "\n",
    "I'm going to show you how I would organize an example project. There are three directories - `code`, `data`, and `paper`. The `code` directory has three programs - one to get the data, one to clean the data and create measures, and one to analyze the data and create visualizations. In more complex projects, this could include lots more programs. Because this class has only used Notebooks, I use Notebooks, but in my work most of these would be .py files, to make it easier to run them from the command line or make them part of pipelines.\n",
    "\n",
    "The `data` directory holds the data. Often, I will have a `raw_data` directory that holds the initial data, and a `cleaned_data` or `summarized_data` directory that represents the measures. Sometimes it's wise to keep even more intermediate datasets.\n",
    "\n",
    "Finally, I'll show how these might fit into a `LaTeX` publishing pipeline. The figures can go straight into a paper, and everything can be updated if the data changes.\n",
    "\n",
    "This project includes a `Makefile`; in this case it just makes the `LaTeX` file into a PDF, but you can make more complicated Makefiles which will automatically update run code as things change. I've also used `snakemake` in the past, which is based on python and is designed for scientific workflows.\n",
    "\n",
    "\n",
    "## Project overview\n",
    "\n",
    "For this simple project, we will try to answer the question, \"In conversations about Purdue, do shorter or longer comments get more upvotes?\"\n",
    "\n",
    "\n",
    "First, we load libraries and authenticate to reddit. Note that you will need to 1) Copy your `reddit_authentication.py` file to the code directory and 2) create a `data` directory (for the raw data to be stored in).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.7.1 of praw is outdated. Version 7.8.1 was released Friday October 25, 2024.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import reddit_authentication\n",
    "from prawcore.exceptions import NotFound\n",
    "import csv\n",
    "\n",
    "# Create an instance called reddit. We'll use this to call the API.\n",
    "reddit = praw.Reddit(client_id=reddit_authentication.client_id,\n",
    "                     client_secret=reddit_authentication.client_secret,\n",
    "                    user_agent = reddit_authentication.user_agent,\n",
    "                    username = reddit_authentication.username,\n",
    "                    password = reddit_authentication.password)\n",
    "\n",
    "fn = '../data/raw_data.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we can't always trust network connections to remain stable, we're going to write out to our file for every post that we look at.\n",
    "\n",
    "This also means that we need to add logic that figures out where we left off and starts at the next post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_posts = 500\n",
    "\n",
    "\n",
    "# Get the list of completed submissions\n",
    "completed = []\n",
    "try:\n",
    "    with open(fn, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if row['submission_id'] not in completed:\n",
    "                completed.append(row['submission_id'])\n",
    "        no_file = False\n",
    "except FileNotFoundError:\n",
    "    completed = []\n",
    "    no_file = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission 10 of 500\n",
      "Submission 20 of 500\n",
      "Submission 30 of 500\n",
      "Submission 40 of 500\n",
      "Submission 50 of 500\n",
      "Submission 60 of 500\n",
      "Submission 70 of 500\n",
      "Submission 80 of 500\n",
      "Submission 90 of 500\n",
      "Submission 100 of 500\n",
      "Submission 110 of 500\n",
      "Submission 120 of 500\n",
      "Submission 130 of 500\n",
      "Submission 140 of 500\n",
      "Submission 150 of 500\n",
      "Submission 160 of 500\n",
      "Submission 170 of 500\n",
      "Submission 180 of 500\n",
      "Submission 190 of 500\n",
      "Submission 200 of 500\n",
      "Submission 210 of 500\n",
      "Submission 220 of 500\n",
      "Submission 230 of 500\n",
      "Submission 240 of 500\n",
      "Submission 250 of 500\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for submission in reddit.subreddit('all').search('Purdue', sort = 'new', limit=num_posts):\n",
    "    comments = []\n",
    "    i += 1\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Submission {i} of {num_posts}\")\n",
    "    if submission.id in completed:\n",
    "        continue\n",
    "    # Get comments\n",
    "    if submission.num_comments == 0:\n",
    "        continue\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    for comment in submission.comments.list():\n",
    "        # I'm skipping deleted or removed comments. You might want to include them.\n",
    "        if comment.author is None:\n",
    "            continue\n",
    "    # At this point, make sure you include every attribute you might possibly need.\n",
    "        comments.append({\n",
    "            'author': comment.author.name,\n",
    "            'body': comment.body,\n",
    "            'created_utc': comment.created_utc,\n",
    "            'id': comment.id,\n",
    "            'link_id': comment.link_id,\n",
    "            'parent_id': comment.parent_id,\n",
    "            'depth': comment.depth,\n",
    "            'score': comment.score,\n",
    "            'score_hidden': comment.score_hidden,\n",
    "            'upvotes': comment.ups,\n",
    "            'downvotes': comment.downs,\n",
    "            'subreddit': comment.subreddit.display_name,\n",
    "            'submission_id': comment.submission.id,\n",
    "            'submission_title': comment.submission.title,\n",
    "            'submission_created_utc': comment.submission.created_utc,\n",
    "            'submission_author': comment.submission.author.name,\n",
    "            'submission_num_comments': comment.submission.num_comments,\n",
    "            'submission_score': comment.submission.score,\n",
    "            'submission_body': comment.submission.selftext,\n",
    "            'submission_url': comment.submission.url\n",
    "        })\n",
    "        completed.append(submission.id)\n",
    "    with open(fn, 'a') as f: # 'a' means append to the file\n",
    "        if len(comments) == 0:\n",
    "            continue\n",
    "        out = csv.DictWriter(f, fieldnames=comments[0].keys())\n",
    "        # If the file is empty, write the header\n",
    "        if no_file:\n",
    "            out.writeheader()\n",
    "            no_file = False\n",
    "        out.writerows(comments)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
