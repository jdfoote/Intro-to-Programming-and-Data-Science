{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and Visualization\n",
    "\n",
    "pandas is a library that is indended to make data analysis easier.\n",
    "\n",
    "The central concept in pandas is a \"dataframe\". These dataframes are similar (in many respects) to Excel spreadsheets. They are typically 2-dimensional representations of data.\n",
    "\n",
    "The first thing that we need to do is to import pandas. By convention, we rename it to \"pd\".\n",
    "\n",
    "We're also importing another visualization library called seaborn. Again, by convention it's imported as \"sns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "For now, we are assuming that you have data from somewhere else, in a [CSV file](https://en.wikipedia.org/wiki/Comma-separated_values). For today's exercises, we're going to use data that I got from [BigQuery](https://console.cloud.google.com/bigquery), which is a tool produced by Google for querying huge datasets.\n",
    "\n",
    "The folks at [Pushshift](https://pushshift.io/) gather and publish a bunch of the data from reddit. I queried their data to get a random set of 100K comments from 2018. You can download that data [here](https://github.com/jdfoote/Intro-to-Programming-and-Data-Science/raw/master/resources/data/100k_random_reddit_2018.csv).\n",
    "\n",
    "First, we load it into a dataframe. Change the path to where you are storing your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv('/home/jeremy/Teaching/intro_to_programming_and_ds/resources/data/100k_random_reddit_2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"head\" method lets us look at the first few rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is organized a lot like a spreadsheet or a CSV file. Each row is an observation, and each column is a measure about that observation. In this case, a row represents a comment on reddit.\n",
    "\n",
    "We are going to be talking a lot more about how to manipulate and work with this data. This document is intended to introduce a few key ideas.\n",
    "\n",
    "## Selecting data\n",
    "\n",
    "The first is selecting data - getting a subset of the data.\n",
    "\n",
    "This is a really confusing thing about pandas and takes some getting used to. There is a detailed explanation [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) but I'm going to try to give a gentler introduction here.\n",
    "\n",
    "The main thing that trips people up is remembering that dataframes have an \"index\". By default, this is a number that refers to each row in the dataframe. If you look at our dataframe, you will see that on the far left is an unnamed column of numbers starting at 0. This is the index.\n",
    "\n",
    "It is assigned in the order that the rows were in the CSV file. However, it can change if we reorder things.\n",
    "\n",
    "For example, this code will sort the dataframe by comment length. Notice how the index numbers are now out of order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_length = reddit_df.sort_values('comment_length', ascending=False)\n",
    "df_by_length.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this \"out of order\" version of the data fram to show the difference between the two main ways of selecting data: `iloc` and `loc`.\n",
    "\n",
    "`iloc` selects data like a list index, based on the location in the current dataframe. The syntax is just like selecting elements from two lists: the row list and the column list. If we use slice notation it looks like this:\n",
    "\n",
    "`dataframe_name.iloc[starting_row:ending_row, staring_column:ending_column]`\n",
    "\n",
    "You can also select a list of row numbers or column numbers. Below, I pass a list of columns to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code gets the first 10 rows and columns 1, 3, and 4\n",
    "df_by_length.iloc[0:10, [1,3,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `loc` syntax, on the other hand, selects based on labels. It selects rows based on index labels and columns based on column names. When something is sorted, this can have unexpected consequences. \n",
    "\n",
    "For example, we might think that the code below would just select the first three rows, or maybe the three rows with indexes 0,1, and 2. Instead, it starts with the item with index `0`, and selects all of the rows from that row until the row with index `3`, as currently sorted! Notice at the bottom of the output, it says that the length is 1,494!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_length.loc[0:3, 'comment_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean indexing\n",
    "\n",
    "Typically, if we want to get a certain number of rows, we use `.iloc`. However, if we want to filter data based on its value, then we typically use `.loc`\n",
    "\n",
    "For example, if we wanted to get the first 3 rows by their index, we'd need to do something like this, which uses \"boolean indexing\" to filter to only the rows where the index is less than or equal to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_length.loc[df_by_length.index < 3,\"comment_length\":]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if we wanted to just get the subreddit and the score whenever scores are above the median score, we could do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_length.loc[df_by_length.score > df_by_length.score.median(),['subreddit', 'score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is possible to filter dataframes without using `iloc` or `loc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One that's useful is dot notation - as long as your column names don't have spaces, you can do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just gives one column\n",
    "df_by_length.score\n",
    "\n",
    "# This is equivalent, and can handle spaces\n",
    "df_by_length['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use slicing notation, this will give you rows (like iloc)\n",
    "\n",
    "df_by_length[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Sort the data frame by score and select the rows with the 10th to 20th highest scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Now, how would you select just the `subreddit` and `comment_length` columns for the rows you selected above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and aggregating\n",
    "\n",
    "Pandas is also great for grouping and aggregating data.\n",
    "\n",
    "We're going to talk a lot more about analyzing data but here's a quick introduciton to some great built in functions like max, sum, and mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean score and comment length\n",
    "\n",
    "df_by_length.loc[:,\"comment_length\":\"score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also might want to create subreddit-level or person-level measures. To do this, we'll want to \"group\" the data.\n",
    "\n",
    "The groupby function will create a \"grouped\" dataframe, where aggregations apply to the group rather than the whole dataframe. I think of this as temporarily sorting all of the rows into their own mini-spreadsheets based on whatever is in the `groupby` function.\n",
    "\n",
    "For example, this will tell use the average comment score by subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that I'm using the original dataframe but either one would work\n",
    "\n",
    "# Groups by subreddit, then gets the score column\n",
    "# Then gets the mean of that column, then sorts it by the value of the column\n",
    "reddit_df.groupby('subreddit').score.median().sort_values() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, this will get the number of comments per person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.groupby('author').size().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Find the average (mean) comment length for each subreddit and sort by comment length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can talk more about this in class, but this creates a user-based dataframe with data for the number of subreddits, number of comments, and average score for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = reddit_df.groupby('author')\n",
    "\n",
    "subreddit_count = grouped_df.subreddit.nunique()\n",
    "comments_count = grouped_df.size()\n",
    "score_mean = grouped_df.score.mean()\n",
    "\n",
    "\n",
    "person_df = pd.DataFrame({'subreddit_count': subreddit_count,\n",
    "                          'comments_count': comments_count,\n",
    "                          'mean_score': score_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "\n",
    "Python has a powerful (and complicated) plotting library called matplotlib. If you want to do more complicated plots, I highly recommend learning how to use it. For now, I'm going to introduce the visualizations that are part of pandas, and later introduce some which are part of Seaborn. Both of these are based on matplotlib but make it easier and simpler to use.\n",
    "\n",
    "I'm introducing a few basic visualizations here. You should [read and reference this page](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html) to learn about a bunch more options.\n",
    "\n",
    "### Scatterplots\n",
    "\n",
    "I think it would be interesting to know if there's a relationship between the length of comments and the score. This plot shows the scatterplot of that relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.plot.scatter(x = 'comment_length', y = 'score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these measures are really skewed, so let's try plotting them on log scales instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that this is slightly different (improved) from the code in the video\n",
    "reddit_df['comment_logged'] = np.log1p(reddit_df.comment_length)\n",
    "# Scores can be negative, so we add the minimum (plus one) to make sure they are positive\n",
    "# And then log transform\n",
    "reddit_df['score_logged'] = np.log1p(reddit_df.score + abs(reddit_df.score.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.plot.scatter(x = 'comment_logged', y = 'score_logged');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of a relationship there.\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "Plot the number of comments and average score per person. Are frequent posters more likely to share comments that are well-received?\n",
    "\n",
    "*Hint*: You will want to use a dataframe that groups the data by person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series\n",
    "\n",
    "The last thing I want to show is how to do time series (because they are confusing!)\n",
    "\n",
    "They key thing to remember is that the plots assume that time is the index of the data frame. By default, the index is just the row number of the row when it was first imported.\n",
    "\n",
    "So, the first thing we need to do is to convert the datetime column to the index.\n",
    "\n",
    "`reddit_df.index` is where the index is stored, so the following code takes the `created_utc` column--which is the [Unix time](https://en.wikipedia.org/wiki/Unix_time) that each comment was posted--and converts it to a \"DateTime\", which is a way of representing a date and time in Python. It then stores the datetime in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.index = pd.to_datetime(reddit_df.created_utc, unit='s', utc=True)\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make a time series, we have to combine the data. The `resample` function does this. Below, we combine it by hour to see daily trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_comments = reddit_df.resample('H').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_comments.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is too messy, so let's zoom in on just a few weeks, using boolean indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_comments.loc[hourly_comments.index < '2018-01-18'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Plot the number of comments per month over the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we wanted to summarize by hour, we could group the posting time by hour. Below, we see that about 8:00 UTC (which is about 3:00 AM Eastern) corresponds to the lowest activity time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.groupby(reddit_df.index.hour).size().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "See if you can figure out how to change the above plot so that instead of showing one overall line for comments per hour, it shows a line for each day of the week (e.g., a line for comments per hour on Mondays, another for Tuesdays, etc.).\n",
    "\n",
    "This code can help you to get started: it sums the number of comments by hour and day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.groupby([reddit_df.index.hour, reddit_df.index.weekday]).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
