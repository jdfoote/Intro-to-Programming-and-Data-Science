{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using APIs in Projects\n",
    "\n",
    "\n",
    "When getting data from APIs, I strongly suggest following a three-step workflow:\n",
    "\n",
    "1. Write some code that gets data from an API and saves all of the data (if possible) to a file\n",
    "2. Write a second program (usually a second file) that loads the data from the API, extracts the data that will be useful for analysis, and saves it in a flat file (typically a CSV).\n",
    "3. Program number 3 loads the CSV file and does the analysis\n",
    "\n",
    "This approach has a few important benefits.\n",
    "\n",
    "The first and most important is that often it is difficult to get the same raw data again. For example, some APIs only lets you get the last week. If you are doing analysis a month down the road and decide that you really wish you had saved different metadata, it is too late. By saving as much of the raw data as possible you can change your measures or analysis strategy in the future (or even do additional studies)\n",
    "\n",
    "The second benefit is that this gives you a nice pipeline, with intermediate files. Instead of including the entire raw data file in the code that does analysis, you only have to load the CSV, which is often much smaller and easier to work with.\n",
    "\n",
    "This brief lesson will show an example of this workflow, using `PRAW`.\n",
    "\n",
    "Note that I'm going to put everything in one file for convenience, but my typical workflow is to put these in separate files and then run each file separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program 1 - Data Retrieval\n",
    "\n",
    "The goal of our project is to characterize the way that people participate in the Purdue subreddit. In particular, we want to create a histogram of the number of posts per person, the number of comments per person, the median comment length per person, and as scatterplot of the relationship between the number of comments and the median comment length.\n",
    "\n",
    "In order to do this, all we really need is to get as many comments as we can from the Purdue subreddit, so that's what our first program will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import reddit_authentication\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# Create an instance called reddit. We'll use this to call the API.\n",
    "reddit = praw.Reddit(client_id=reddit_authentication.client_id,\n",
    "                     client_secret=reddit_authentication.client_secret,\n",
    "                    user_agent = reddit_authentication.user_agent,\n",
    "                    username = reddit_authentication.username,\n",
    "                    password = reddit_authentication.password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be a better approach, but we're going to grab all of the posts (called submissions), and then get all of the comments for each post.\n",
    "\n",
    "We're also going to save the data as we go, so that if we need to stop, we can pick up where we left off.\n",
    "\n",
    "This is a little bit complicated, but we're going to save two files: one that is a list of all of the submissions we've sucessfully retrieved, and one that actually contains all of the comments. I'm doing this because sometimes the amount of data you have is so large that you don't want to keep it all in memory, you just want to write it out as quickly as possible.\n",
    "\n",
    "Ideally, we want to keep the data as close to raw as possible; PRAW gives us an object, which isn't easy to save. So we'll have to select the attributes we want to keep, and save these in a CSV file. But, I'm going to save everything I might possibly want.\n",
    "\n",
    "Unfortunately, I learned that we can only get up to 1,000 submissions, so we'll get the top 1,000 over the last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11lj5h1\n",
      "12yszks\n",
      "xz6nl8\n",
      "ynwr9k\n",
      "11qwfau\n",
      "xy46ne\n",
      "11ayx4k\n",
      "yvcpel\n",
      "10s6opx\n",
      "zn2sms\n",
      "104w0hs\n",
      "13lyd93\n",
      "yw2e2p\n",
      "zudghk\n",
      "zfm715\n",
      "zbj9ea\n",
      "10nuxql\n",
      "yhzby5\n",
      "13b3pe3\n",
      "y35x4d\n",
      "y1cgg3\n",
      "11ueoat\n",
      "yzlmfj\n",
      "10uwym0\n",
      "zkyioe\n",
      "yqt4pk\n",
      "10uv7py\n",
      "zlcue0\n",
      "15eg7yd\n",
      "z7kqrx\n",
      "1563qtg\n",
      "zsst3s\n",
      "yt9dat\n",
      "yxds0m\n",
      "13k53ud\n",
      "116u60j\n",
      "10c4x3k\n",
      "12075vj\n",
      "10cpomd\n",
      "ynhuxz\n",
      "11c2x57\n",
      "10xwrep\n",
      "137b3zx\n",
      "yuizs7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('./submissions.csv', 'w') as f:\n",
    "    out = csv.writer(f)\n",
    "    out.writerow(['id', 'title', 'author', 'created_utc', 'comments_retrieved'])\n",
    "    for submission in reddit.subreddit('Purdue').top(limit=None, time_filter = 'year'):\n",
    "        try:\n",
    "            name = submission.author.name\n",
    "        except AttributeError:\n",
    "            name = None\n",
    "            print(submission)\n",
    "        out.writerow([submission.id, submission.title, name, submission.created_utc, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can just load that submissions file, so we don't need to run that code again.\n",
    "\n",
    "The cool thing about this code is that it's written so that you can stop it and start running it again. It will pick up where it left off.\n",
    "\n",
    "Sometimes, you will be running code that runs for hours or days (or longer), and having checkpointing like this can be really important.\n",
    "\n",
    "Indeed, I received a network error while running this code, and it's likely that you will as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving comments for xuq0zp\n",
      "Retrieving comments for 16ttgp6\n",
      "Retrieving comments for 14n6bvj\n",
      "Retrieving comments for 13lzzj2\n",
      "Retrieving comments for 11hcy4j\n",
      "Retrieving comments for 11893cq\n",
      "Retrieving comments for 10y4i5m\n",
      "Retrieving comments for zgpbgb\n",
      "Retrieving comments for z8osj6\n",
      "Retrieving comments for z5ockv\n",
      "Retrieving comments for yuizs7\n",
      "Retrieving comments for 14xq5c7\n",
      "Retrieving comments for 12zzjpl\n",
      "Retrieving comments for 12z2dyu\n",
      "Retrieving comments for 11dq1jg\n",
      "Retrieving comments for 10ugx8t\n",
      "Retrieving comments for zpxn1q\n",
      "Retrieving comments for zoyikk\n",
      "Retrieving comments for 13jja1b\n",
      "Retrieving comments for 1398jmy\n",
      "Retrieving comments for 126tbwm\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./submissions.csv')\n",
    "\n",
    "# Check if the output file exists. If not, create it and write the header.\n",
    "\n",
    "if not os.path.exists('./comments.csv'):\n",
    "    with open('./comments.csv', 'w') as f:\n",
    "        out = csv.writer(f)\n",
    "        out.writerow(['id',\n",
    "                      'body',\n",
    "                      'author', \n",
    "                      'created_utc', \n",
    "                      'parent_id', \n",
    "                      'submission_id', \n",
    "                      'tot_awards_received', \n",
    "                      'ups', \n",
    "                      'downs', \n",
    "                      'score'])\n",
    "\n",
    "for submission_id in df.loc[df.comments_retrieved == False, 'id']:\n",
    "    print(f'Retrieving comments for {submission_id}')\n",
    "    submission = reddit.submission(id=submission_id)\n",
    "    # This sets the limit to None, which means that it will retrieve all comments.\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    # Because we're only storing whether a submission was retrieved, we save all the comments and write them at the same time.\n",
    "    curr_comments = []\n",
    "    for comment in submission.comments.list():\n",
    "        try:\n",
    "            name = comment.author.name\n",
    "        except AttributeError:\n",
    "            name = None\n",
    "        curr_comments.append([comment.id, \n",
    "                        comment.body, \n",
    "                        name, \n",
    "                        comment.created_utc, \n",
    "                        comment.parent_id,\n",
    "                        submission.id,\n",
    "                        comment.total_awards_received,\n",
    "                        comment.ups,\n",
    "                        comment.downs,\n",
    "                        comment.score\n",
    "                        ])\n",
    "    with open('./comments.csv', 'a') as f:\n",
    "        out = csv.writer(f)\n",
    "        out.writerows(curr_comments)\n",
    "    df.loc[df.id == submission_id, 'comments_retrieved'] = True\n",
    "    df.to_csv('./submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program 2 - Data Cleaning\n",
    "\n",
    "This program loads the saved raw data. Here, we grab what we want, create new measures, and save it to a new CSV.\n",
    "\n",
    "We need to get posts per person, comments per person, and median comment length per person.\n",
    "\n",
    "Pandas is really good at this, so we'll use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.read_csv('./comments.csv')\n",
    "\n",
    "comments_df['comment_length'] = comments_df.body.str.len()\n",
    "\n",
    "commenter_stats = comments_df.groupby('author').agg(\n",
    "    # Number of comments\n",
    "    num_comments = ('id', 'count'),\n",
    "    # Median comment length\n",
    "    median_comment_length = ('comment_length', 'median'),\n",
    "    # Median score\n",
    "    median_score = ('score', 'median'),\n",
    ").reset_index()\n",
    "\n",
    "# Now, we need to grab the number of posts from the other CSV file, and merge the two together.\n",
    "\n",
    "submissions_df = pd.read_csv('./submissions.csv')\n",
    "\n",
    "submitter_stats = submissions_df.groupby('author').agg(\n",
    "    num_posts = ('id', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Now, we can merge the two together.\n",
    "merged_df = pd.merge(commenter_stats, submitter_stats, on='author', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>tot_awards_received</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>score</th>\n",
       "      <th>comment_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jgovki5</td>\n",
       "      <td>Why no slander ab Purdue's WiFi lmao</td>\n",
       "      <td>HanTheMan34</td>\n",
       "      <td>1.681782e+09</td>\n",
       "      <td>t3_12q15fj</td>\n",
       "      <td>12q15fj</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jgpave7</td>\n",
       "      <td>Definitely made by an aero major lol</td>\n",
       "      <td>McLegendd</td>\n",
       "      <td>1.681789e+09</td>\n",
       "      <td>t3_12q15fj</td>\n",
       "      <td>12q15fj</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jgowpuz</td>\n",
       "      <td>This was well made. Solid effort. C- (A+ with ...</td>\n",
       "      <td>wes00mertes</td>\n",
       "      <td>1.681783e+09</td>\n",
       "      <td>t3_12q15fj</td>\n",
       "      <td>12q15fj</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jgofus3</td>\n",
       "      <td>Industrial “engineering”</td>\n",
       "      <td>Seth4832</td>\n",
       "      <td>1.681775e+09</td>\n",
       "      <td>t3_12q15fj</td>\n",
       "      <td>12q15fj</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jgos590</td>\n",
       "      <td>Waltuh</td>\n",
       "      <td>Anfers2</td>\n",
       "      <td>1.681781e+09</td>\n",
       "      <td>t3_12q15fj</td>\n",
       "      <td>12q15fj</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24853</th>\n",
       "      <td>jeug4tb</td>\n",
       "      <td>Spot on. But they don't tell you this lol.</td>\n",
       "      <td>whatup_pips</td>\n",
       "      <td>1.680561e+09</td>\n",
       "      <td>t1_jeuenf0</td>\n",
       "      <td>126tbwm</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24854</th>\n",
       "      <td>jemndd5</td>\n",
       "      <td>I hope you’re right but as an international st...</td>\n",
       "      <td>annsparklebby</td>\n",
       "      <td>1.680414e+09</td>\n",
       "      <td>t1_jel69g4</td>\n",
       "      <td>126tbwm</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24855</th>\n",
       "      <td>jeb38u8</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>Isa_The_iguana_2023</td>\n",
       "      <td>1.680202e+09</td>\n",
       "      <td>t1_jeaxi3b</td>\n",
       "      <td>126tbwm</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24856</th>\n",
       "      <td>jec4r3g</td>\n",
       "      <td>dispite knowing about the housing situation th...</td>\n",
       "      <td>Isa_The_iguana_2023</td>\n",
       "      <td>1.680217e+09</td>\n",
       "      <td>t1_jec2cmi</td>\n",
       "      <td>126tbwm</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24857</th>\n",
       "      <td>jec69mj</td>\n",
       "      <td>That's why I stayed 🙃</td>\n",
       "      <td>whatup_pips</td>\n",
       "      <td>1.680217e+09</td>\n",
       "      <td>t1_jec4r3g</td>\n",
       "      <td>126tbwm</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24858 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               body  \\\n",
       "0      jgovki5               Why no slander ab Purdue's WiFi lmao   \n",
       "1      jgpave7               Definitely made by an aero major lol   \n",
       "2      jgowpuz  This was well made. Solid effort. C- (A+ with ...   \n",
       "3      jgofus3                           Industrial “engineering”   \n",
       "4      jgos590                                             Waltuh   \n",
       "...        ...                                                ...   \n",
       "24853  jeug4tb         Spot on. But they don't tell you this lol.   \n",
       "24854  jemndd5  I hope you’re right but as an international st...   \n",
       "24855  jeb38u8                                              Yeah.   \n",
       "24856  jec4r3g  dispite knowing about the housing situation th...   \n",
       "24857  jec69mj                              That's why I stayed 🙃   \n",
       "\n",
       "                    author   created_utc   parent_id submission_id  \\\n",
       "0              HanTheMan34  1.681782e+09  t3_12q15fj       12q15fj   \n",
       "1                McLegendd  1.681789e+09  t3_12q15fj       12q15fj   \n",
       "2              wes00mertes  1.681783e+09  t3_12q15fj       12q15fj   \n",
       "3                 Seth4832  1.681775e+09  t3_12q15fj       12q15fj   \n",
       "4                  Anfers2  1.681781e+09  t3_12q15fj       12q15fj   \n",
       "...                    ...           ...         ...           ...   \n",
       "24853          whatup_pips  1.680561e+09  t1_jeuenf0       126tbwm   \n",
       "24854        annsparklebby  1.680414e+09  t1_jel69g4       126tbwm   \n",
       "24855  Isa_The_iguana_2023  1.680202e+09  t1_jeaxi3b       126tbwm   \n",
       "24856  Isa_The_iguana_2023  1.680217e+09  t1_jec2cmi       126tbwm   \n",
       "24857          whatup_pips  1.680217e+09  t1_jec4r3g       126tbwm   \n",
       "\n",
       "       tot_awards_received  ups  downs  score  comment_length  \n",
       "0                        0   81      0     81            36.0  \n",
       "1                        0   60      0     60            36.0  \n",
       "2                        0  139      0    139           166.0  \n",
       "3                        0   86      0     86            24.0  \n",
       "4                        0   26      0     26             6.0  \n",
       "...                    ...  ...    ...    ...             ...  \n",
       "24853                    0    1      0      1            42.0  \n",
       "24854                    0    1      0      1           176.0  \n",
       "24855                    0    1      0      1             5.0  \n",
       "24856                    0    1      0      1           109.0  \n",
       "24857                    0    2      0      2            21.0  \n",
       "\n",
       "[24858 rows x 11 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>median_comment_length</th>\n",
       "      <th>median_score</th>\n",
       "      <th>num_posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3950</th>\n",
       "      <td>itakeskypics</td>\n",
       "      <td>52</td>\n",
       "      <td>55.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>Wheatley312</td>\n",
       "      <td>29</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>AggressiveAd8587</td>\n",
       "      <td>4</td>\n",
       "      <td>49.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>SelfRedeemedBoiler</td>\n",
       "      <td>134</td>\n",
       "      <td>73.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>Its-Mike-Jones</td>\n",
       "      <td>231</td>\n",
       "      <td>68.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5172</th>\n",
       "      <td>zootia</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5173</th>\n",
       "      <td>zoro412</td>\n",
       "      <td>4</td>\n",
       "      <td>22.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5174</th>\n",
       "      <td>zspacer</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5175</th>\n",
       "      <td>zuckjeet</td>\n",
       "      <td>1</td>\n",
       "      <td>67.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5176</th>\n",
       "      <td>zwroberts15</td>\n",
       "      <td>18</td>\n",
       "      <td>206.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5177 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author  num_comments  median_comment_length  median_score  \\\n",
       "3950        itakeskypics            52                   55.0           7.0   \n",
       "2919         Wheatley312            29                   88.0           5.0   \n",
       "152     AggressiveAd8587             4                   49.0          33.5   \n",
       "2385  SelfRedeemedBoiler           134                   73.5           2.0   \n",
       "1323      Its-Mike-Jones           231                   68.0           4.0   \n",
       "...                  ...           ...                    ...           ...   \n",
       "5172              zootia             1                   28.0           4.0   \n",
       "5173             zoro412             4                   22.5           4.0   \n",
       "5174             zspacer             1                   37.0          41.0   \n",
       "5175            zuckjeet             1                   67.0          17.0   \n",
       "5176         zwroberts15            18                  206.5          25.0   \n",
       "\n",
       "      num_posts  \n",
       "3950       23.0  \n",
       "2919       17.0  \n",
       "152        16.0  \n",
       "2385       15.0  \n",
       "1323       12.0  \n",
       "...         ...  \n",
       "5172        NaN  \n",
       "5173        NaN  \n",
       "5174        NaN  \n",
       "5175        NaN  \n",
       "5176        NaN  \n",
       "\n",
       "[5177 rows x 5 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.sort_values('num_posts', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our cleaed data to a CSV file.\n",
    "\n",
    "merged_df.to_csv('./cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program 3 - Data Analysis\n",
    "\n",
    "Here we use pandas to load the data and analyze it. This could include statistical tests. Here, I'm just visualizing the distribution of posts, comments, and comment length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just make sure it looks OK.\n",
    "df.sort_values('num_comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='num_posts', data = df, binwidth=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='num_comments', data = df, binwidth=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, these are both super skewed, with most people only commenting or posting once, while a few commented a ton.\n",
    "\n",
    "Let's see if it changes if we get rid of people who only commented once (maybe we have a principled reason to believe they are different than other users)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.loc[df.num_comments > 1, 'num_comments'], binwidth=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I thought, this is a somewhat \"scale-free\" distribution, meaning wherever you zoom in, you see the same pattern. Try changing the `1` up above to any (small) number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment length and number of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(y='num_comments', x='median_comment_length', data = df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both of these are so skewed, so let's log them\n",
    "p = sns.jointplot(y=np.log(df.num_comments), x=np.log(df.median_comment_length), data = df, kind = 'reg')\n",
    "p.set_axis_labels(xlabel= 'Median comment length (logged)', ylabel='Number of comments (logged)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does appear to be a correlation between the number of comments and the median comment length. This is interesting, and suggests that people who comment a lot tend to write longer comments.\n",
    "\n",
    "For fun, let's also look at the relationship between the number of comments and the median score. Ths might be an explanation for our findings: if people who comment a lot tend to get more upvotes, then they might be more likely to comment more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='median_score', data = df[df.num_comments > 5], binwidth=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a logged median score (hard because it can be negative)\n",
    "\n",
    "df['logged_median_score'] = np.log(df.median_score + abs(df.median_score.min()) + 2)\n",
    "\n",
    "p = sns.jointplot(y=np.log(df.num_comments), x='logged_median_score', data = df, kind = 'reg')\n",
    "p.set_axis_labels(xlabel= 'Median score (logged)', ylabel='Number of comments (logged)');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
