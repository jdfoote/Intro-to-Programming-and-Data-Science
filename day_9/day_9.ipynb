{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using APIs in Projects\n",
    "\n",
    "\n",
    "When getting data from APIs, I strongly suggest following a three-step workflow:\n",
    "\n",
    "1. Write some code that gets data from an API and saves all of the data (if possible) to a file\n",
    "2. Write a second program (usually a second file) that loads the data from the API, extracts the data that will be useful for analysis, and saves it in a flat file (typically a CSV).\n",
    "3. Program number 3 loads the CSV file and does the analysis\n",
    "\n",
    "This approach has a few important benefits.\n",
    "\n",
    "The first and most important is that often it is difficult to get the same raw data again. For example, some APIs only lets you get the last week. If you are doing analysis a month down the road and decide that you really wish you had saved different metadata, it is too late. By saving as much of the raw data as possible you can change your measures or analysis strategy in the future (or even do additional studies)\n",
    "\n",
    "The second benefit is that this gives you a nice pipeline, with intermediate files. Instead of including the entire raw data file in the code that does analysis, you only have to load the CSV, which is often much smaller and easier to work with.\n",
    "\n",
    "This brief lesson will show an example of this workflow, using `PRAW`.\n",
    "\n",
    "Note that I'm going to put everything in one file for convenience, but my typical workflow is to put these in separate files and then run each file separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program 1 - Data Retrieval\n",
    "\n",
    "The goal of our project is to characterize the way that people participate in the Purdue subreddit. In particular, we want to create a histogram of the number of posts per person, the number of comments per person, the median comment length per person, and as scatterplot of the relationship between the number of comments and the median comment length.\n",
    "\n",
    "In order to do this, all we really need is to get as many comments as we can from the Purdue subreddit, so that's what our first program will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import reddit_authentication\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# Create an instance called reddit. We'll use this to call the API.\n",
    "reddit = praw.Reddit(client_id=reddit_authentication.client_id,\n",
    "                     client_secret=reddit_authentication.client_secret,\n",
    "                    user_agent = reddit_authentication.user_agent,\n",
    "                    username = reddit_authentication.username,\n",
    "                    password = reddit_authentication.password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be a better approach, but we're going to grab all of the posts (called submissions), and then get all of the comments for each post.\n",
    "\n",
    "We're also going to save the data as we go, so that if we need to stop, we can pick up where we left off.\n",
    "\n",
    "This is a little bit complicated, but we're going to save two files: one that is a list of all of the submissions we've sucessfully retrieved, and one that actually contains all of the comments. I'm doing this because sometimes the amount of data you have is so large that you don't want to keep it all in memory, you just want to write it out as quickly as possible.\n",
    "\n",
    "Ideally, we want to keep the data as close to raw as possible; PRAW gives us an object, which isn't easy to save. So we'll have to select the attributes we want to keep, and save these in a CSV file. But, I'm going to save everything I might possibly want.\n",
    "\n",
    "Unfortunately, I learned that we can only get up to 1,000 submissions, so we'll get the top 1,000 over the last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./submissions.csv', 'w') as f:\n",
    "    out = csv.writer(f)\n",
    "    out.writerow(['id', 'title', 'author', 'created_utc', 'comments_retrieved'])\n",
    "    for submission in reddit.subreddit('Purdue').top(limit=None, time_filter = 'year'):\n",
    "        try:\n",
    "            name = submission.author.name\n",
    "        except AttributeError:\n",
    "            name = None\n",
    "            print(submission)\n",
    "        out.writerow([submission.id, submission.title, name, submission.created_utc, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can just load that submissions file, so we don't need to run that code again.\n",
    "\n",
    "The cool thing about this code is that it's written so that you can stop it and start running it again. It will pick up where it left off.\n",
    "\n",
    "Sometimes, you will be running code that runs for hours or days (or longer), and having checkpointing like this can be really important.\n",
    "\n",
    "Indeed, I received a network error while running this code, and it's likely that you will as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./submissions.csv')\n",
    "\n",
    "# Check if the output file exists. If not, create it and write the header.\n",
    "\n",
    "if not os.path.exists('./comments.csv'):\n",
    "    with open('./comments.csv', 'w') as f:\n",
    "        out = csv.writer(f)\n",
    "        out.writerow(['id',\n",
    "                      'body',\n",
    "                      'author', \n",
    "                      'created_utc', \n",
    "                      'parent_id', \n",
    "                      'submission_id', \n",
    "                      'tot_awards_received', \n",
    "                      'ups', \n",
    "                      'downs', \n",
    "                      'score'])\n",
    "\n",
    "for submission_id in df.loc[df.comments_retrieved == False, 'id']:\n",
    "    print(f'Retrieving comments for {submission_id}')\n",
    "    submission = reddit.submission(id=submission_id)\n",
    "    # This sets the limit to None, which means that it will retrieve all comments.\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    # Because we're only storing whether a submission was retrieved, we save all the comments and write them at the same time.\n",
    "    curr_comments = []\n",
    "    for comment in submission.comments.list():\n",
    "        try:\n",
    "            name = comment.author.name\n",
    "        except AttributeError:\n",
    "            name = None\n",
    "        curr_comments.append([comment.id, \n",
    "                        comment.body, \n",
    "                        name, \n",
    "                        comment.created_utc, \n",
    "                        comment.parent_id,\n",
    "                        submission.id,\n",
    "                        comment.total_awards_received,\n",
    "                        comment.ups,\n",
    "                        comment.downs,\n",
    "                        comment.score\n",
    "                        ])\n",
    "    with open('./comments.csv', 'a') as f:\n",
    "        out = csv.writer(f)\n",
    "        out.writerows(curr_comments)\n",
    "    df.loc[df.id == submission_id, 'comments_retrieved'] = True\n",
    "    df.to_csv('./submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program 2 - Data Cleaning\n",
    "\n",
    "This program loads the saved raw data. Here, we grab what we want, create new measures, and save it to a new CSV.\n",
    "\n",
    "We need to get posts per person, comments per person, and median comment length per person.\n",
    "\n",
    "Pandas is really good at this, so we'll use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.read_csv('./comments.csv')\n",
    "\n",
    "comments_df['comment_length'] = comments_df.body.str.len()\n",
    "\n",
    "commenter_stats = comments_df.groupby('author').agg(\n",
    "    # Number of comments\n",
    "    num_comments = ('id', 'count'),\n",
    "    # Median comment length\n",
    "    median_comment_length = ('comment_length', 'median'),\n",
    "    # Median score\n",
    "    median_score = ('score', 'median'),\n",
    ").reset_index()\n",
    "\n",
    "# Now, we need to grab the number of posts from the other CSV file, and merge the two together.\n",
    "\n",
    "submissions_df = pd.read_csv('./submissions.csv')\n",
    "\n",
    "submitter_stats = submissions_df.groupby('author').agg(\n",
    "    num_posts = ('id', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Now, we can merge the two together.\n",
    "merged_df = pd.merge(commenter_stats, submitter_stats, on='author', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.sort_values('num_posts', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our cleaed data to a CSV file.\n",
    "\n",
    "merged_df.to_csv('./cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program 3 - Data Analysis\n",
    "\n",
    "Here we use pandas to load the data and analyze it. This could include statistical tests. Here, I'm just visualizing the distribution of posts, comments, and comment length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just make sure it looks OK.\n",
    "df.sort_values('num_comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='num_posts', data = df, binwidth=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='num_comments', data = df, binwidth=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, these are both super skewed, with most people only commenting or posting once, while a few commented a ton.\n",
    "\n",
    "Let's see if it changes if we get rid of people who only commented once (maybe we have a principled reason to believe they are different than other users)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.loc[df.num_comments > 1, 'num_comments'], binwidth=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I thought, this is a somewhat \"scale-free\" distribution, meaning wherever you zoom in, you see the same pattern. Try changing the `1` up above to any (small) number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment length and number of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(y='num_comments', x='median_comment_length', data = df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both of these are so skewed, so let's log them\n",
    "p = sns.jointplot(y=np.log(df.num_comments), x=np.log(df.median_comment_length), data = df, kind = 'reg')\n",
    "p.set_axis_labels(xlabel= 'Median comment length (logged)', ylabel='Number of comments (logged)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does appear to be a correlation between the number of comments and the median comment length. This is interesting, and suggests that people who comment a lot tend to write longer comments.\n",
    "\n",
    "For fun, let's also look at the relationship between the number of comments and the median score. Ths might be an explanation for our findings: if people who comment a lot tend to get more upvotes, then they might be more likely to comment more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='median_score', data = df, binwidth=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a logged median score (hard because it can be negative)\n",
    "\n",
    "df['logged_median_score'] = np.log(df.median_score + abs(df.median_score.min()) + 2)\n",
    "\n",
    "p = sns.jointplot(y=np.log(df.num_comments), x='logged_median_score', data = df, kind = 'reg')\n",
    "p.set_axis_labels(xlabel= 'Median score (logged)', ylabel='Number of comments (logged)');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
