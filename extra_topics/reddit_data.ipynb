{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reddit Data for Social Scientists\n",
    "\n",
    "[Reddit](https://www.reddit.com) is one of the most popular websites in the world. It is composed of hundreds of thousands of \"subreddits\" - topical communities which users subscribe to. These communities are then aggregated into a personalized home page for each user.\n",
    "\n",
    "The size and influence of Reddit have made it the focus of an increasing amount of social science scholarship. In addition, it is by far the most open of the major social networking sites, with full text comments and information about users made available via API.\n",
    "\n",
    "There are currently two main approaches to getting data from Reddit. The first is through the reddit API. The API itself is powerful but really confusing to get into. I highly recommend the use of [praw](https://praw.readthedocs.io/en/latest/), a really great \"wrapper\" for the API (similar to tweepy for Twitter). As with tweepy, you will need to install it through conda or pip.\n",
    "\n",
    "In order to use the Reddit API, you need to create a reddit account, and then [create an application](https://www.reddit.com/prefs/apps/). This is basically the same as what we did with tweepy - you will be given a secret, which must be used in connection with the username and password for the account.\n",
    "\n",
    "These should be stored in their own file, which you are careful not to accidentally upload to Github! :)\n",
    "\n",
    "In the same directory as the notebook you use to run this code, create a file called `reddit_auth.py` with the following information (replace with actual strings from the application you create):\n",
    "\n",
    "```\n",
    "client_id = 'aaaaaaaaa'\n",
    "secret = 'bbbbbbbbb'\n",
    "user = 'ccccccccc'\n",
    "pw = 'ddddddddd'\n",
    "```\n",
    "\n",
    "Then, we authenticate.\n",
    "\n",
    "You can use praw to do all sorts of interaction with the site - writing comments or posts, moderating things, etc. I am just going to show data collection, which only requires read-only rights. For these actions, you actually don't need to pass your user name and password. So it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import reddit_auth\n",
    "\n",
    "# Create an instance called reddit. We'll use this to call the API.\n",
    "reddit = praw.Reddit(client_id=reddit_auth.client_id,\n",
    "                     client_secret=reddit_auth.secret,\n",
    "                     user_agent=\"Example for COM 674 class\") # Call this something descriptive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are a few examples of basic things you might want to do.\n",
    "\n",
    "First, getting recent posts from a set of subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's find the top 10 COVID-related subreddits, according to reddit's search\n",
    "top_covid_subs = [x for x in reddit.subreddits.search('Coronavirus')][:10]\n",
    "\n",
    "# Each of these is a subreddit object, and we can get information about the subreddit from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in top_covid_subs:\n",
    "    print(f\"Name: {s.display_name}\\tSubscribers: {s.subscribers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we can also iterate through each subreddit and get comments\n",
    "\n",
    "comment_dict = {}\n",
    "\n",
    "for sr in top_covid_subs:\n",
    "    curr_comments_text = []\n",
    "    curr_sr_comments = sr.comments(limit=500)\n",
    "    for comment in curr_sr_comments:\n",
    "        curr_comments_text.append(comment.body)\n",
    "    comment_dict[sr.display_name] = curr_comments_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other thing we might want is information about users. Here's a simple example of how to do that for the users who made the last 10 comments in the \"Purdue\" subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = []\n",
    "for c in reddit.subreddit('Purdue').comments(limit=10):\n",
    "    authors.append(c.author)\n",
    "    \n",
    "for author in authors:\n",
    "    print(f\"Name: {author.name}\\t Comment karma: {author.comment_karma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushshift.io\n",
    "\n",
    "The other option for reddit data is [Pushshift](https://github.com/pushshift/api). Jason Baumgartner has been archiving every comment and post on reddit, and allows researchers access to this data in a way that is often much more convenient and faster than the reddit API. Also, because it's only public data, there is no need for authentication.\n",
    "\n",
    "It also includes some cool tools for aggregating data but for this example, I'm just showing grabbing data.\n",
    "\n",
    "Pushshift is really organized just around submissions and comments - it's designed for getting reddit-level or subreddit-level data about how much content is being produced around topics. It does not, for example, let you get direct informationa about users. Nor can you directly navigate a comment tree, like the Reddit API lets you do. However, it's really great for many common use cases.\n",
    "\n",
    "_Note:_ There is a wrapper for Pushshift called `psaw`. It's not much easier than just using Pushshift directly, IMO, so i'm just showing how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you do the same thing - get the last 500 comments from each of the subreddits in our list\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Initial URL\n",
    "endpt = \"https://api.pushshift.io/reddit/search/comment\"\n",
    "\n",
    "\n",
    "comment_dict = {}\n",
    "for sr in top_covid_subs:\n",
    "    params = {'subreddit': sr.display_name,\n",
    "              'size': 500} # Get 500 comments for each\n",
    "    r = requests.get(endpt, params = params)\n",
    "    # Take the reqeusts object, and extract just the text\n",
    "    comment_dict[sr.display_name] = [x['body'] for x in r.json()['data']]\n",
    "    # Add a sleep, to be kind to the API (really only needed if you are getting a lot of data)\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need more than 500 comments, you will need a while loop.\n",
    "\n",
    "endpt = \"https://api.pushshift.io/reddit/search/comment\"\n",
    "\n",
    "comment_dict = {}\n",
    "before = ''\n",
    "for sr in top_covid_subs[:2]:\n",
    "    curr_comments = []\n",
    "    while len(curr_comments) < 2000:\n",
    "        params = {'subreddit': sr.display_name,\n",
    "                  'before': before,\n",
    "                  'size': 500} # Get 500 comments for each\n",
    "        r = requests.get(endpt, params = params)\n",
    "        # Take the reqeusts object, and extract just the text\n",
    "        curr_comments += [x['body'] for x in r.json()['data']]\n",
    "        # Get the created time of the last comment; for the next run, only get \n",
    "        # the comments older than that\n",
    "        before = r.json()['data'][-1]['created_utc'] \n",
    "    # Add a sleep, to be kind to the API (really only needed if you are getting a lot of data)\n",
    "    comment_dict[sr.display_name] = curr_comments\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comment_dict['Coronavirus'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
