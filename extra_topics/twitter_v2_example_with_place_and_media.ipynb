{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter V2 Full Archive Search\n",
    "\n",
    "This document shows how to use Tweepy to conduct a full archive search using v2 of the Twitter API.\n",
    "\n",
    "## Prep work\n",
    "\n",
    "In order to use this code, you will need to have a developer account on Twitter, with access to the Academic Research product track. Information about who is eligible and how to apply is [here](https://developer.twitter.com/en/products/twitter-api/academic-research).\n",
    "\n",
    "Once you have an account, you will need to create a new app at https://developer.twitter.com/en/portal/dashboard and generate a \"bearer token\" from the app. Copy the bearer token to your clipboard and paste it into a new file in the same directory as this file, called `twitter_authentication.py`. The entire contents of the file should look like this:\n",
    "\n",
    "```python\n",
    "bearer_token = \"YOUR BEARER TOKEN HERE\"\n",
    "```\n",
    "\n",
    "Note that you should **never** share this token with anyone else. If, for example, you are saving your work in a Git repository, make sure that you add the `twitter_authentication.py` file to your `.gitignore`.\n",
    "\n",
    "If anyone gets this token, they will have access to your Twitter account and you will need to revoke the token (from the same interface where you created it).\n",
    "\n",
    "If you've created the file successfully, then the following two blocks of code should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from twitter_authentication import bearer_token\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Search API\n",
    "\n",
    "Full documentation for searching tweets is at https://docs.tweepy.org/en/latest/client.html#search-tweets. There are a lot of different options, but here is a simple version that gets all of the \"COVID hoax\" tweets from January 10, 2021. \n",
    "\n",
    "By default the only information returned is the tweet ID and the text. Often, we will want information about authors, too. To get information about the author, you need to add the `user_fields` parameter with the fields you want as well as the `expansions = 'author_id'` parameter. \n",
    "\n",
    "To get more information about the tweet, you need the `tweet_fields` parameter. The options are shown at https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all\n",
    "\n",
    "You also likely want to build a somewhat advanced query - instructions are at https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query. For this query, I get English language tweets that are not retweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoax_tweets = []\n",
    "for response in tweepy.Paginator(client.search_all_tweets, \n",
    "                                 query = 'COVID hoax -is:retweet lang:en',\n",
    "                                 user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "                                 tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                 media_fields = 'type',\n",
    "                                 expansions = ['author_id', 'attachments.media_keys'],\n",
    "                                 start_time = '2021-01-20T00:00:00Z',\n",
    "                                 end_time = '2022-01-21T00:00:00Z',\n",
    "                              max_results=500, limit = 2):\n",
    "    time.sleep(1)\n",
    "    hoax_tweets.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hoax_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I followed the best practice above of saving the raw response returned. If this were a real project, I would write out all of the raw responses into a file. For long-running queries (e.g., if you need to get hundreds of thousands of tweets), you will often want to build in some error handling and a way to resume data collection. For example, you might write all of the results to a file and then open the file, retrieve the last tweet, and use the ID of that tweet to tell the script where to start to retrieve new tweets.\n",
    "\n",
    "The other problem is that the object that is returned is a bit confusing - it is nested, with the tweet data in `.data` and the user data in `.includes['users']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tweet id=1484314594247987203 text=@AnfelisaSpiorad But I though Covid was a hoax?>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoax_tweets[0].data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['users', 'media'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoax_tweets[0].includes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<User id=2642608841 name=Mercy username=Mercington_>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoax_tweets[0].includes['users'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both of these are objects. The data that we asked for in `user_fields` and `tweet_fields` above are attributes of the objects. For example, here's the user's description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Catholic Christian ‚úùÔ∏è, Libertarian+Conservative=Me, üá∫üá∏ üá≤üáΩ üáµüá∑ , In that order Also also also typo man, learning ASL, 21'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoax_tweets[0].includes['users'][2].description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will often want to reorganize these into a flat file, which means connecting a tweet to the user data of the user who wrote it. I show an example of how to do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "user_dict = {}\n",
    "# Loop through each response object\n",
    "for response in hoax_tweets:\n",
    "    # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "    for user in response.includes['users']:\n",
    "        user_dict[user.id] = {'username': user.username, \n",
    "                              'followers': user.public_metrics['followers_count'],\n",
    "                              'tweets': user.public_metrics['tweet_count'],\n",
    "                              'description': user.description,\n",
    "                              'location': user.location\n",
    "                             }\n",
    "    for tweet in response.data:\n",
    "        # For each tweet, find the author's information\n",
    "        author_info = user_dict[tweet.author_id]\n",
    "        # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "        result.append({'author_id': tweet.author_id, \n",
    "                       'username': author_info['username'],\n",
    "                       'author_followers': author_info['followers'],\n",
    "                       'author_tweets': author_info['tweets'],\n",
    "                       'author_description': author_info['description'],\n",
    "                       'author_location': author_info['location'],\n",
    "                       'text': tweet.text,\n",
    "                       'created_at': tweet.created_at,\n",
    "                       'retweets': tweet.public_metrics['retweet_count'],\n",
    "                       'replies': tweet.public_metrics['reply_count'],\n",
    "                       'likes': tweet.public_metrics['like_count'],\n",
    "                       'quote_count': tweet.public_metrics['quote_count']\n",
    "                      })\n",
    "\n",
    "# Change this list of dictionaries into a dataframe\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>username</th>\n",
       "      <th>author_followers</th>\n",
       "      <th>author_tweets</th>\n",
       "      <th>author_description</th>\n",
       "      <th>author_location</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweets</th>\n",
       "      <th>replies</th>\n",
       "      <th>likes</th>\n",
       "      <th>quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1173999122791055360</td>\n",
       "      <td>LardFDorkness2</td>\n",
       "      <td>1596</td>\n",
       "      <td>16148</td>\n",
       "      <td>1st LardFDorkness account banned for gettin' s...</td>\n",
       "      <td>None</td>\n",
       "      <td>@larrycharlesism ALSO, the mitigation measures...</td>\n",
       "      <td>2021-01-20 23:57:46+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>733606999</td>\n",
       "      <td>james_thomas127</td>\n",
       "      <td>16</td>\n",
       "      <td>7283</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>@covidhoax2020 @KevinVanAusdal LOLOL your name...</td>\n",
       "      <td>2021-01-20 23:55:40+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>862336513</td>\n",
       "      <td>CallOfDove</td>\n",
       "      <td>279</td>\n",
       "      <td>18561</td>\n",
       "      <td>Catholic Christian ‚úùÔ∏è, Libertarian+Conservativ...</td>\n",
       "      <td>My heart is in Rome (Midwest)</td>\n",
       "      <td>@SimonMichaelPa2 @cristhian_0707 @guypbenson I...</td>\n",
       "      <td>2021-01-20 23:54:40+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>798360156559982592</td>\n",
       "      <td>Since_U_Asked</td>\n",
       "      <td>1770</td>\n",
       "      <td>29511</td>\n",
       "      <td>I like ez hikes following streams\\nthen cozy n...</td>\n",
       "      <td>Obama saved economy from Bush</td>\n",
       "      <td>400.000 soldiers buried in Arlington\\n400,000 ...</td>\n",
       "      <td>2021-01-20 23:53:59+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23139663</td>\n",
       "      <td>suzanneb1123</td>\n",
       "      <td>1025</td>\n",
       "      <td>10057</td>\n",
       "      <td>wear a mask &amp; don‚Äôt be racist</td>\n",
       "      <td>Orange County, CA</td>\n",
       "      <td>Ok, so now that all you Q Anon followers have ...</td>\n",
       "      <td>2021-01-20 23:50:40+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>2909962150</td>\n",
       "      <td>BobinSea</td>\n",
       "      <td>28</td>\n",
       "      <td>5934</td>\n",
       "      <td>BELIEVE what Politicians DO and not what they ...</td>\n",
       "      <td>who cares?</td>\n",
       "      <td>FEB 2020  -- FOX HOSTS CALL COVID a \"HOAX\"\\n\\n...</td>\n",
       "      <td>2021-01-20 00:04:49+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>1326533089405833216</td>\n",
       "      <td>AlwaysVoteTruth</td>\n",
       "      <td>68</td>\n",
       "      <td>15047</td>\n",
       "      <td>MPA praying for our democracy back in the USA</td>\n",
       "      <td>None</td>\n",
       "      <td>@RandPaul You meant to say the Trump regime th...</td>\n",
       "      <td>2021-01-20 00:02:58+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>59075703</td>\n",
       "      <td>panji90</td>\n",
       "      <td>355</td>\n",
       "      <td>13785</td>\n",
       "      <td>Rants, commentaries, shitposts, and much less ...</td>\n",
       "      <td>Cempakaputihindah, Indonesia</td>\n",
       "      <td>A very good resource on Covid-19. It explained...</td>\n",
       "      <td>2021-01-20 00:01:54+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>24393759</td>\n",
       "      <td>soma77</td>\n",
       "      <td>134</td>\n",
       "      <td>32806</td>\n",
       "      <td>The presenter, John Kuykendall lived as a monk...</td>\n",
       "      <td>Sparks, Nevada</td>\n",
       "      <td>Remembering Covid victims, Biden spends emotio...</td>\n",
       "      <td>2021-01-20 00:01:39+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>601434382</td>\n",
       "      <td>Rfotofolio</td>\n",
       "      <td>987</td>\n",
       "      <td>6023</td>\n",
       "      <td>Our mission is to give photographers access to...</td>\n",
       "      <td>United States</td>\n",
       "      <td>@Yamiche 400000 Americans dead from Covid .. t...</td>\n",
       "      <td>2021-01-20 00:01:21+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>913 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               author_id         username  author_followers  author_tweets  \\\n",
       "0    1173999122791055360   LardFDorkness2              1596          16148   \n",
       "1              733606999  james_thomas127                16           7283   \n",
       "2              862336513       CallOfDove               279          18561   \n",
       "3     798360156559982592    Since_U_Asked              1770          29511   \n",
       "4               23139663     suzanneb1123              1025          10057   \n",
       "..                   ...              ...               ...            ...   \n",
       "908           2909962150         BobinSea                28           5934   \n",
       "909  1326533089405833216  AlwaysVoteTruth                68          15047   \n",
       "910             59075703          panji90               355          13785   \n",
       "911             24393759           soma77               134          32806   \n",
       "912            601434382       Rfotofolio               987           6023   \n",
       "\n",
       "                                    author_description  \\\n",
       "0    1st LardFDorkness account banned for gettin' s...   \n",
       "1                                                        \n",
       "2    Catholic Christian ‚úùÔ∏è, Libertarian+Conservativ...   \n",
       "3    I like ez hikes following streams\\nthen cozy n...   \n",
       "4                        wear a mask & don‚Äôt be racist   \n",
       "..                                                 ...   \n",
       "908  BELIEVE what Politicians DO and not what they ...   \n",
       "909      MPA praying for our democracy back in the USA   \n",
       "910  Rants, commentaries, shitposts, and much less ...   \n",
       "911  The presenter, John Kuykendall lived as a monk...   \n",
       "912  Our mission is to give photographers access to...   \n",
       "\n",
       "                   author_location  \\\n",
       "0                             None   \n",
       "1                             None   \n",
       "2    My heart is in Rome (Midwest)   \n",
       "3    Obama saved economy from Bush   \n",
       "4                Orange County, CA   \n",
       "..                             ...   \n",
       "908                    who cares?    \n",
       "909                           None   \n",
       "910   Cempakaputihindah, Indonesia   \n",
       "911                 Sparks, Nevada   \n",
       "912                  United States   \n",
       "\n",
       "                                                  text  \\\n",
       "0    @larrycharlesism ALSO, the mitigation measures...   \n",
       "1    @covidhoax2020 @KevinVanAusdal LOLOL your name...   \n",
       "2    @SimonMichaelPa2 @cristhian_0707 @guypbenson I...   \n",
       "3    400.000 soldiers buried in Arlington\\n400,000 ...   \n",
       "4    Ok, so now that all you Q Anon followers have ...   \n",
       "..                                                 ...   \n",
       "908  FEB 2020  -- FOX HOSTS CALL COVID a \"HOAX\"\\n\\n...   \n",
       "909  @RandPaul You meant to say the Trump regime th...   \n",
       "910  A very good resource on Covid-19. It explained...   \n",
       "911  Remembering Covid victims, Biden spends emotio...   \n",
       "912  @Yamiche 400000 Americans dead from Covid .. t...   \n",
       "\n",
       "                   created_at  retweets  replies  likes  quote_count  \n",
       "0   2021-01-20 23:57:46+00:00         0        0      1            0  \n",
       "1   2021-01-20 23:55:40+00:00         0        0      0            0  \n",
       "2   2021-01-20 23:54:40+00:00         0        0      0            0  \n",
       "3   2021-01-20 23:53:59+00:00         0        0      0            0  \n",
       "4   2021-01-20 23:50:40+00:00         0        0      0            0  \n",
       "..                        ...       ...      ...    ...          ...  \n",
       "908 2021-01-20 00:04:49+00:00         0        0      0            0  \n",
       "909 2021-01-20 00:02:58+00:00         0        0      0            0  \n",
       "910 2021-01-20 00:01:54+00:00         1        0      0            0  \n",
       "911 2021-01-20 00:01:39+00:00         0        0      0            0  \n",
       "912 2021-01-20 00:01:21+00:00         0        0      0            0  \n",
       "\n",
       "[913 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `requests`-based version\n",
    "\n",
    "If you want to do things without tweepy, here is some boilerplate code that should work. As you can see, it's much more complicated. Be grateful for the tweepy developers!! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import twitter_authentication as config\n",
    "import time\n",
    "\n",
    "# Save your bearer token in a file called twitter_authentication.py in this directory\n",
    "# Should look like this:\n",
    "# bearer_token = 'YOUR_BEARER_TOKEN_HERE'\n",
    "\n",
    "bearer_token = config.bearer_token\n",
    "query = '(#COVID) OR (#COVID-19)'\n",
    "out_file = 'raw_tweets.txt'\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "# Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "# expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
    "query_params = {'query': query,\n",
    "                'start_time': '2010-01-01T12:00:00Z',\n",
    "                'tweet.fields': 'author_id,public_metrics',\n",
    "                 'user.fields': 'username',\n",
    "                'expansions': 'author_id',\n",
    "                'max_results': 500\n",
    "               }\n",
    "\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    if next_token:\n",
    "        params['next_token'] = next_token\n",
    "    response = requests.request(\"GET\", search_url, headers=headers, params=params)\n",
    "    time.sleep(3.1)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_tweets(num_tweets, output_fh):\n",
    "    next_token = None\n",
    "    tweets_stored = 0\n",
    "    while tweets_stored < num_tweets:\n",
    "        headers = create_headers(bearer_token)\n",
    "        json_response = connect_to_endpoint(search_url, headers, query_params, next_token)\n",
    "        if json_response['meta']['result_count'] == 0:\n",
    "            break\n",
    "        author_dict = {x['id']: x['username'] for x in json_response['includes']['users']}\n",
    "        for tweet in json_response['data']:\n",
    "            try:\n",
    "                tweet['username'] = author_dict[tweet['author_id']]\n",
    "            except KeyError:\n",
    "                print(f\"No data for {tweet['author_id']}\")\n",
    "            output_fh.write(json.dumps(tweet) + '\\n')\n",
    "            tweets_stored += 1\n",
    "        try:\n",
    "            next_token = json_response['meta']['next_token']\n",
    "        except KeyError:\n",
    "            break\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(out_file, 'w') as f:\n",
    "        get_tweets(500, f)\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "with open(out_file, 'r') as f:\n",
    "    for row in f.readlines():\n",
    "        tweet = json.loads(row)\n",
    "        tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
