{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter V2 Full Archive Search\n",
    "\n",
    "This document shows how to use Tweepy to conduct a full archive search using v2 of the Twitter API.\n",
    "\n",
    "## Prep work\n",
    "\n",
    "In order to use this code, you will need to have a developer account on Twitter, with access to the Academic Research product track. Information about who is eligible and how to apply is [here](https://developer.twitter.com/en/products/twitter-api/academic-research).\n",
    "\n",
    "Once you have an account, you will need to create a new app at https://developer.twitter.com/en/portal/dashboard and generate a \"bearer token\" from the app. Copy the bearer token to your clipboard and paste it into a new file in the same directory as this file, called `twitter_authentication.py`. The entire contents of the file should look like this:\n",
    "\n",
    "```python\n",
    "bearer_token = \"YOUR BEARER TOKEN HERE\"\n",
    "```\n",
    "\n",
    "Note that you should **never** share this token with anyone else. If, for example, you are saving your work in a Git repository, make sure that you add the `twitter_authentication.py` file to your `.gitignore`.\n",
    "\n",
    "If anyone gets this token, they will have access to your Twitter account and you will need to revoke the token (from the same interface where you created it).\n",
    "\n",
    "If you've created the file successfully, then the following two blocks of code should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from twitter_authentication import bearer_token\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Search API\n",
    "\n",
    "Full documentation for searching tweets is at https://docs.tweepy.org/en/latest/client.html#search-tweets. There are a lot of different options, but here is a simple version that gets all of the \"COVID hoax\" tweets from January 10, 2021. \n",
    "\n",
    "By default the only information returned is the tweet ID and the text. Often, we will want information about authors, too. To get information about the author, you need to add the `user_fields` parameter with the fields you want as well as the `expansions = 'author_id'` parameter. \n",
    "\n",
    "To get more information about the tweet, you need the `tweet_fields` parameter. The options are shown at https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all\n",
    "\n",
    "You also likely want to build a somewhat advanced query - instructions are at https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query. For this query, I get English language tweets that are not retweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoax_tweets = []\n",
    "for response in tweepy.Paginator(client.search_all_tweets, \n",
    "                                 query = 'COVID hoax -is:retweet lang:en',\n",
    "                                 user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "                                 tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                 media_fields = 'type',\n",
    "                                 place_fields = ['place_type', 'name', 'country'],\n",
    "                                 expansions = ['author_id', 'attachments.media_keys', 'geo.place_id'],\n",
    "                                 start_time = '2021-01-20T00:00:00Z',\n",
    "                                 end_time = '2022-09-21T00:00:00Z',\n",
    "                              max_results=500, limit = 2):\n",
    "    time.sleep(1)\n",
    "    hoax_tweets.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hoax_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I followed the best practice above of saving the raw response returned. If this were a real project, I would write out all of the raw responses into a file. For long-running queries (e.g., if you need to get hundreds of thousands of tweets), you will often want to build in some error handling and a way to resume data collection. For example, you might write all of the results to a file and then open the file, retrieve the last tweet, and use the ID of that tweet to tell the script where to start to retrieve new tweets.\n",
    "\n",
    "The other problem is that the object that is returned is a bit confusing - it is nested, with the tweet data in `.data` with extra user, media, and location data in `.includes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tweet id=1572374312358907906 text=RT@MagaRisingJohn  NO, because it's an even bigger hoax then Covid??? https://t.co/JQBv9PpEFS>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoax_tweets[0].data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's in `includes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['users', 'places', 'media'])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoax_tweets[0].includes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'media_key': '3_1572359846674006017', 'type': 'photo'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoax_tweets[0].includes['media'][0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both of these are objects. The data that we asked for in `user_fields` and `tweet_fields` above are attributes of the objects. For example, here's the user's description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have experienced life in America for 69 years.  Believe in God, Country and a free way of life.'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoax_tweets[0].includes['users'][0].description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will often want to reorganize these into a flat file, which means connecting a tweet to the includes data. I show an example of how to do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "user_dict = {}\n",
    "places_dict = {}\n",
    "media_dict = {}\n",
    "# Loop through each response object\n",
    "for response in hoax_tweets:\n",
    "    # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "    for user in response.includes['users']:\n",
    "        user_dict[user.id] = {'username': user.username, \n",
    "                              'followers': user.public_metrics['followers_count'],\n",
    "                              'tweets': user.public_metrics['tweet_count'],\n",
    "                              'description': user.description,\n",
    "                              'location': user.location\n",
    "                             }\n",
    "    \n",
    "    for location in response.includes['places']:\n",
    "        places_dict[location.id] = {'country': location.country,\n",
    "                                    'name': location.full_name,\n",
    "                                    'place_type': location.place_type\n",
    "                                   }\n",
    "        \n",
    "    for media in response.includes['media']:\n",
    "        media_dict[media.media_key] = {'type': media.type}\n",
    "    \n",
    "    for tweet in response.data:\n",
    "        tweet_dict = {}\n",
    "        has_video = False\n",
    "        has_photo = False\n",
    "        # For each tweet, find the extra information\n",
    "        try:\n",
    "            author_info = user_dict[tweet.author_id]\n",
    "            tweet_dict.update({'username': author_info['username'],\n",
    "                       'author_followers': author_info['followers'],\n",
    "                       'author_tweets': author_info['tweets'],\n",
    "                       'author_description': author_info['description'],\n",
    "                       'author_location': author_info['location']})\n",
    "            \n",
    "        except AtributeError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            place_info = places_dict[tweet.geo['place_id']]\n",
    "            tweet_dict.update({'tweet_location': place_info['name'],\n",
    "                             'tweet_country': place_info['country'],\n",
    "                             'tweet_location_type': place_info['place_type']})\n",
    "        except TypeError:\n",
    "            pass\n",
    "            \n",
    "\n",
    "        try:\n",
    "            for key in tweet.attachments['media_keys']:\n",
    "                media_type = media_dict[key]['type']\n",
    "                if media_type == 'photo':\n",
    "                    has_photo = True\n",
    "                if media_type == 'video':\n",
    "                    has_video = True\n",
    "        except TypeError:\n",
    "            pass\n",
    "        \n",
    "        # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "        tweet_dict.update({'author_id': tweet.author_id, \n",
    "                       'text': tweet.text,\n",
    "                       'created_at': tweet.created_at,\n",
    "                       'retweets': tweet.public_metrics['retweet_count'],\n",
    "                       'replies': tweet.public_metrics['reply_count'],\n",
    "                       'likes': tweet.public_metrics['like_count'],\n",
    "                       'quote_count': tweet.public_metrics['quote_count'],\n",
    "                        'has_photo': has_photo,\n",
    "                        'has_video': has_video\n",
    "                      })\n",
    "        result.append(tweet_dict)\n",
    "# Change this list of dictionaries into a dataframe\n",
    "df = pd.DataFrame(result, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>author_followers</th>\n",
       "      <th>author_tweets</th>\n",
       "      <th>author_description</th>\n",
       "      <th>author_location</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweets</th>\n",
       "      <th>replies</th>\n",
       "      <th>likes</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>has_photo</th>\n",
       "      <th>has_video</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>tweet_country</th>\n",
       "      <th>tweet_location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JuliaJett8</td>\n",
       "      <td>255</td>\n",
       "      <td>19004</td>\n",
       "      <td>I have experienced life in America for 69 year...</td>\n",
       "      <td>None</td>\n",
       "      <td>1240743593561616384</td>\n",
       "      <td>RT@MagaRisingJohn  NO, because it's an even bi...</td>\n",
       "      <td>2022-09-20 23:57:04+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>andycpp6</td>\n",
       "      <td>16</td>\n",
       "      <td>229</td>\n",
       "      <td>Mom of 1 enjoys crafts with the kids, scrapboo...</td>\n",
       "      <td>Toronto, canada</td>\n",
       "      <td>1605642564</td>\n",
       "      <td>@StevenJS_ @CityNewsTO i  lost a family member...</td>\n",
       "      <td>2022-09-20 23:51:05+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TheVictoryTour</td>\n",
       "      <td>2628</td>\n",
       "      <td>70951</td>\n",
       "      <td>#PromiseKeepersüá∫üá∏ #YesWeCan~Victory requires V...</td>\n",
       "      <td>United States</td>\n",
       "      <td>105917268</td>\n",
       "      <td>Floridians should also file a class action sui...</td>\n",
       "      <td>2022-09-20 23:50:44+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ReadDavidCase</td>\n",
       "      <td>4585</td>\n",
       "      <td>27555</td>\n",
       "      <td>Exec Producer #EXPLANT doc, #howsyourheadhun B...</td>\n",
       "      <td>L.A.</td>\n",
       "      <td>115034110</td>\n",
       "      <td>@RepAndyBiggsAZ And once again a Republican ci...</td>\n",
       "      <td>2022-09-20 23:46:14+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joesegal</td>\n",
       "      <td>10452</td>\n",
       "      <td>241543</td>\n",
       "      <td>Expand circles of kindness and compassion\\nins...</td>\n",
       "      <td>LA, USA</td>\n",
       "      <td>16548484</td>\n",
       "      <td>@jayrosen_nyu @dnbornstein I think there has t...</td>\n",
       "      <td>2022-09-20 23:37:30+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>KonaBean5</td>\n",
       "      <td>1247</td>\n",
       "      <td>30235</td>\n",
       "      <td>Sick of GOP BS &amp; greed. I loathe TFG and his c...</td>\n",
       "      <td>Virginia, USA</td>\n",
       "      <td>838115678200741888</td>\n",
       "      <td>@Socott2030 Mask mandates also stopped people ...</td>\n",
       "      <td>2022-09-16 17:09:30+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>TexasFree4</td>\n",
       "      <td>229</td>\n",
       "      <td>37116</td>\n",
       "      <td>We love our family, friends &amp; the USA. We're a...</td>\n",
       "      <td>None</td>\n",
       "      <td>1518714002951573504</td>\n",
       "      <td>@POTUS You didn't have any problem not letting...</td>\n",
       "      <td>2022-09-16 17:06:27+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>d_aids</td>\n",
       "      <td>594</td>\n",
       "      <td>3596</td>\n",
       "      <td>Enough about me. Join millions who have flippe...</td>\n",
       "      <td>Hollywood, California</td>\n",
       "      <td>4710680126</td>\n",
       "      <td>@NickAdamsinUSA The country hasn't been this d...</td>\n",
       "      <td>2022-09-16 17:03:57+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>OMGno2trump</td>\n",
       "      <td>115670</td>\n",
       "      <td>43489</td>\n",
       "      <td>I'm an educated guy who cares about the world ...</td>\n",
       "      <td>I live in a small house in Charlotte, NC area</td>\n",
       "      <td>800111181058838528</td>\n",
       "      <td>@Jim_Jordan Because your COVID hoax killed ove...</td>\n",
       "      <td>2022-09-16 16:58:09+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>HeyItTae</td>\n",
       "      <td>119</td>\n",
       "      <td>10047</td>\n",
       "      <td>tae?? ‚Ä¢ 22 ‚Ä¢ queer ‚Ä¢ she/they ‚Ä¢ perpetual idio...</td>\n",
       "      <td>‚ú®Ô∏èüçú@SyberSuccubiüçú‚ú®Ô∏è</td>\n",
       "      <td>1166118486692659201</td>\n",
       "      <td>ppl who say covid is a hoax or \"isn't that bad...</td>\n",
       "      <td>2022-09-16 16:49:09+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>945 rows √ó 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           username  author_followers  author_tweets  \\\n",
       "0        JuliaJett8               255          19004   \n",
       "1          andycpp6                16            229   \n",
       "2    TheVictoryTour              2628          70951   \n",
       "3     ReadDavidCase              4585          27555   \n",
       "4          joesegal             10452         241543   \n",
       "..              ...               ...            ...   \n",
       "940       KonaBean5              1247          30235   \n",
       "941      TexasFree4               229          37116   \n",
       "942          d_aids               594           3596   \n",
       "943     OMGno2trump            115670          43489   \n",
       "944        HeyItTae               119          10047   \n",
       "\n",
       "                                    author_description  \\\n",
       "0    I have experienced life in America for 69 year...   \n",
       "1    Mom of 1 enjoys crafts with the kids, scrapboo...   \n",
       "2    #PromiseKeepersüá∫üá∏ #YesWeCan~Victory requires V...   \n",
       "3    Exec Producer #EXPLANT doc, #howsyourheadhun B...   \n",
       "4    Expand circles of kindness and compassion\\nins...   \n",
       "..                                                 ...   \n",
       "940  Sick of GOP BS & greed. I loathe TFG and his c...   \n",
       "941  We love our family, friends & the USA. We're a...   \n",
       "942  Enough about me. Join millions who have flippe...   \n",
       "943  I'm an educated guy who cares about the world ...   \n",
       "944  tae?? ‚Ä¢ 22 ‚Ä¢ queer ‚Ä¢ she/they ‚Ä¢ perpetual idio...   \n",
       "\n",
       "                                   author_location            author_id  \\\n",
       "0                                             None  1240743593561616384   \n",
       "1                                  Toronto, canada           1605642564   \n",
       "2                                    United States            105917268   \n",
       "3                                             L.A.            115034110   \n",
       "4                                          LA, USA             16548484   \n",
       "..                                             ...                  ...   \n",
       "940                                  Virginia, USA   838115678200741888   \n",
       "941                                           None  1518714002951573504   \n",
       "942                          Hollywood, California           4710680126   \n",
       "943  I live in a small house in Charlotte, NC area   800111181058838528   \n",
       "944                            ‚ú®Ô∏èüçú@SyberSuccubiüçú‚ú®Ô∏è  1166118486692659201   \n",
       "\n",
       "                                                  text  \\\n",
       "0    RT@MagaRisingJohn  NO, because it's an even bi...   \n",
       "1    @StevenJS_ @CityNewsTO i  lost a family member...   \n",
       "2    Floridians should also file a class action sui...   \n",
       "3    @RepAndyBiggsAZ And once again a Republican ci...   \n",
       "4    @jayrosen_nyu @dnbornstein I think there has t...   \n",
       "..                                                 ...   \n",
       "940  @Socott2030 Mask mandates also stopped people ...   \n",
       "941  @POTUS You didn't have any problem not letting...   \n",
       "942  @NickAdamsinUSA The country hasn't been this d...   \n",
       "943  @Jim_Jordan Because your COVID hoax killed ove...   \n",
       "944  ppl who say covid is a hoax or \"isn't that bad...   \n",
       "\n",
       "                   created_at  retweets  replies  likes  quote_count  \\\n",
       "0   2022-09-20 23:57:04+00:00         0        0      0            0   \n",
       "1   2022-09-20 23:51:05+00:00         0       16     66            0   \n",
       "2   2022-09-20 23:50:44+00:00         0        0      0            0   \n",
       "3   2022-09-20 23:46:14+00:00         0        0      0            0   \n",
       "4   2022-09-20 23:37:30+00:00         0        0      0            0   \n",
       "..                        ...       ...      ...    ...          ...   \n",
       "940 2022-09-16 17:09:30+00:00         0        2      0            0   \n",
       "941 2022-09-16 17:06:27+00:00         0        0      0            0   \n",
       "942 2022-09-16 17:03:57+00:00         0        0      0            0   \n",
       "943 2022-09-16 16:58:09+00:00         3        0      8            0   \n",
       "944 2022-09-16 16:49:09+00:00         0        0      0            0   \n",
       "\n",
       "     has_photo  has_video tweet_location tweet_country tweet_location_type  \n",
       "0        False      False            NaN           NaN                 NaN  \n",
       "1        False      False            NaN           NaN                 NaN  \n",
       "2        False      False            NaN           NaN                 NaN  \n",
       "3        False      False            NaN           NaN                 NaN  \n",
       "4        False      False            NaN           NaN                 NaN  \n",
       "..         ...        ...            ...           ...                 ...  \n",
       "940      False      False            NaN           NaN                 NaN  \n",
       "941      False      False            NaN           NaN                 NaN  \n",
       "942      False      False            NaN           NaN                 NaN  \n",
       "943      False      False            NaN           NaN                 NaN  \n",
       "944      False      False            NaN           NaN                 NaN  \n",
       "\n",
       "[945 rows x 17 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `requests`-based version\n",
    "\n",
    "If you want to do things without tweepy, here is some boilerplate code that should work. As you can see, it's much more complicated. Be grateful for the tweepy developers!! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import twitter_authentication as config\n",
    "import time\n",
    "\n",
    "# Save your bearer token in a file called twitter_authentication.py in this directory\n",
    "# Should look like this:\n",
    "# bearer_token = 'YOUR_BEARER_TOKEN_HERE'\n",
    "\n",
    "bearer_token = config.bearer_token\n",
    "query = '(#COVID) OR (#COVID-19)'\n",
    "out_file = 'raw_tweets.txt'\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "# Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "# expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
    "query_params = {'query': query,\n",
    "                'start_time': '2010-01-01T12:00:00Z',\n",
    "                'tweet.fields': 'author_id,public_metrics',\n",
    "                 'user.fields': 'username',\n",
    "                'expansions': 'author_id',\n",
    "                'max_results': 500\n",
    "               }\n",
    "\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    if next_token:\n",
    "        params['next_token'] = next_token\n",
    "    response = requests.request(\"GET\", search_url, headers=headers, params=params)\n",
    "    time.sleep(3.1)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_tweets(num_tweets, output_fh):\n",
    "    next_token = None\n",
    "    tweets_stored = 0\n",
    "    while tweets_stored < num_tweets:\n",
    "        headers = create_headers(bearer_token)\n",
    "        json_response = connect_to_endpoint(search_url, headers, query_params, next_token)\n",
    "        if json_response['meta']['result_count'] == 0:\n",
    "            break\n",
    "        author_dict = {x['id']: x['username'] for x in json_response['includes']['users']}\n",
    "        for tweet in json_response['data']:\n",
    "            try:\n",
    "                tweet['username'] = author_dict[tweet['author_id']]\n",
    "            except KeyError:\n",
    "                print(f\"No data for {tweet['author_id']}\")\n",
    "            output_fh.write(json.dumps(tweet) + '\\n')\n",
    "            tweets_stored += 1\n",
    "        try:\n",
    "            next_token = json_response['meta']['next_token']\n",
    "        except KeyError:\n",
    "            break\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(out_file, 'w') as f:\n",
    "        get_tweets(500, f)\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "with open(out_file, 'r') as f:\n",
    "    for row in f.readlines():\n",
    "        tweet = json.loads(row)\n",
    "        tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
