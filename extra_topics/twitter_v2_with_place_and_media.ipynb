{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter V2 Full Archive Search\n",
    "\n",
    "This document shows how to use Tweepy to conduct a full archive search using v2 of the Twitter API.\n",
    "\n",
    "## Prep work\n",
    "\n",
    "In order to use this code, you will need to have a developer account on Twitter, with access to the Academic Research product track. Information about who is eligible and how to apply is [here](https://developer.twitter.com/en/products/twitter-api/academic-research).\n",
    "\n",
    "Once you have an account, you will need to create a new app at https://developer.twitter.com/en/portal/dashboard and generate a \"bearer token\" from the app. Copy the bearer token to your clipboard and paste it into a new file in the same directory as this file, called `twitter_authentication.py`. The entire contents of the file should look like this:\n",
    "\n",
    "```python\n",
    "bearer_token = \"YOUR BEARER TOKEN HERE\"\n",
    "```\n",
    "\n",
    "Note that you should **never** share this token with anyone else. If, for example, you are saving your work in a Git repository, make sure that you add the `twitter_authentication.py` file to your `.gitignore`.\n",
    "\n",
    "If anyone gets this token, they will have access to your Twitter account and you will need to revoke the token (from the same interface where you created it).\n",
    "\n",
    "If you've created the file successfully, then the following two blocks of code should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from twitter_authentication import bearer_token\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Search API\n",
    "\n",
    "Full documentation for searching tweets is at https://docs.tweepy.org/en/latest/client.html#search-tweets. There are a lot of different options, but here is a simple version that gets all of the \"COVID hoax\" tweets from January 10, 2021. \n",
    "\n",
    "By default the only information returned is the tweet ID and the text. Often, we will want information about authors, too. To get information about the author, you need to add the `user_fields` parameter with the fields you want as well as the `expansions = 'author_id'` parameter. \n",
    "\n",
    "To get more information about the tweet, you need the `tweet_fields` parameter. The options are shown at https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all\n",
    "\n",
    "You also likely want to build a somewhat advanced query - instructions are at https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query. For this query, I get English language tweets that are not retweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoax_tweets = []\n",
    "for response in tweepy.Paginator(client.search_all_tweets, \n",
    "                                 query = 'COVID hoax -is:retweet lang:en',\n",
    "                                 user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "                                 tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                 media_fields = 'type',\n",
    "                                 place_fields = ['place_type', 'name', 'country'],\n",
    "                                 expansions = ['author_id', 'attachments.media_keys', 'geo.place_id'],\n",
    "                                 start_time = '2021-01-20T00:00:00Z',\n",
    "                                 end_time = '2022-09-21T00:00:00Z',\n",
    "                              max_results=500, limit = 2):\n",
    "    time.sleep(1)\n",
    "    hoax_tweets.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hoax_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I followed the best practice above of saving the raw response returned. If this were a real project, I would write out all of the raw responses into a file. For long-running queries (e.g., if you need to get hundreds of thousands of tweets), you will often want to build in some error handling and a way to resume data collection. For example, you might write all of the results to a file and then open the file, retrieve the last tweet, and use the ID of that tweet to tell the script where to start to retrieve new tweets.\n",
    "\n",
    "The other problem is that the object that is returned is a bit confusing - it is nested, with the tweet data in `.data` with extra user, media, and location data in `.includes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1572374312358907906',\n",
       " 'text': \"RT@MagaRisingJohn  NO, because it's an even bigger hoax then Covid??? https://t.co/JQBv9PpEFS\",\n",
       " 'author_id': '1240743593561616384',\n",
       " 'created_at': '2022-09-20T23:57:04.000Z',\n",
       " 'public_metrics': {'retweet_count': 0,\n",
       "  'reply_count': 0,\n",
       "  'like_count': 0,\n",
       "  'quote_count': 0,\n",
       "  'impression_count': 0},\n",
       " 'edit_history_tweet_ids': ['1572374312358907906']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoax_tweets[0].data[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's in `includes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['users', 'places', 'media'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoax_tweets[0].includes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0113afc024d5e0bc',\n",
       " 'full_name': 'Wellington, FL',\n",
       " 'place_type': 'city',\n",
       " 'country': 'United States',\n",
       " 'name': 'Wellington'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoax_tweets[0].includes['places'][0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both of these are objects. The data that we asked for in `user_fields` and `tweet_fields` above are attributes of the objects. For example, here's the user's description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoax_tweets[0].includes['users'][0].description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will often want to reorganize these into a flat file, which means connecting a tweet to the includes data. I show an example of how to do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "user_dict = {}\n",
    "places_dict = {}\n",
    "media_dict = {}\n",
    "# Loop through each response object\n",
    "for response in hoax_tweets:\n",
    "    # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "    for user in response.includes['users']:\n",
    "        user_dict[user.id] = {'username': user.username, \n",
    "                              'followers': user.public_metrics['followers_count'],\n",
    "                              'tweets': user.public_metrics['tweet_count'],\n",
    "                              'description': user.description,\n",
    "                              'location': user.location\n",
    "                             }\n",
    "    \n",
    "    for location in response.includes['places']:\n",
    "        places_dict[location.id] = {'country': location.country,\n",
    "                                    'name': location.full_name,\n",
    "                                    'place_type': location.place_type\n",
    "                                   }\n",
    "        \n",
    "    for media in response.includes['media']:\n",
    "        media_dict[media.media_key] = {'type': media.type}\n",
    "    \n",
    "    for tweet in response.data:\n",
    "        tweet_dict = {}\n",
    "        has_video = False\n",
    "        has_photo = False\n",
    "        # For each tweet, find the extra information\n",
    "        try:\n",
    "            author_info = user_dict[tweet.author_id]\n",
    "            tweet_dict.update({'username': author_info['username'],\n",
    "                       'author_followers': author_info['followers'],\n",
    "                       'author_tweets': author_info['tweets'],\n",
    "                       'author_description': author_info['description'],\n",
    "                       'author_location': author_info['location']})\n",
    "            \n",
    "        except AtributeError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            place_info = places_dict[tweet.geo['place_id']]\n",
    "            tweet_dict.update({'tweet_location': place_info['name'],\n",
    "                             'tweet_country': place_info['country'],\n",
    "                             'tweet_location_type': place_info['place_type']})\n",
    "        except TypeError:\n",
    "            pass\n",
    "            \n",
    "\n",
    "        try:\n",
    "            for key in tweet.attachments['media_keys']:\n",
    "                media_type = media_dict[key]['type']\n",
    "                if media_type == 'photo':\n",
    "                    has_photo = True\n",
    "                if media_type == 'video':\n",
    "                    has_video = True\n",
    "        except TypeError:\n",
    "            pass\n",
    "        \n",
    "        # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "        tweet_dict.update({'author_id': tweet.author_id, \n",
    "                       'text': tweet.text,\n",
    "                       'created_at': tweet.created_at,\n",
    "                       'retweets': tweet.public_metrics['retweet_count'],\n",
    "                       'replies': tweet.public_metrics['reply_count'],\n",
    "                       'likes': tweet.public_metrics['like_count'],\n",
    "                       'quote_count': tweet.public_metrics['quote_count'],\n",
    "                        'has_photo': has_photo,\n",
    "                        'has_video': has_video\n",
    "                      })\n",
    "        result.append(tweet_dict)\n",
    "# Change this list of dictionaries into a dataframe\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>author_followers</th>\n",
       "      <th>author_tweets</th>\n",
       "      <th>author_description</th>\n",
       "      <th>author_location</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweets</th>\n",
       "      <th>replies</th>\n",
       "      <th>likes</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>has_photo</th>\n",
       "      <th>has_video</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>tweet_country</th>\n",
       "      <th>tweet_location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pe62050408</td>\n",
       "      <td>898</td>\n",
       "      <td>12215</td>\n",
       "      <td></td>\n",
       "      <td>Florida, USA</td>\n",
       "      <td>1456667468588535813</td>\n",
       "      <td>I thought Covid was a hoax? What’s with all th...</td>\n",
       "      <td>2022-09-20 23:13:24+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Wellington, FL</td>\n",
       "      <td>United States</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>AustenMu</td>\n",
       "      <td>367</td>\n",
       "      <td>10754</td>\n",
       "      <td>Mom, wife, NICU RN, rescue kitten foster. I wr...</td>\n",
       "      <td>Springfield, MO</td>\n",
       "      <td>1265682481</td>\n",
       "      <td>@RolandMazzucch3 @curtquin @stormscomming @abr...</td>\n",
       "      <td>2022-09-20 20:51:58+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Missouri, USA</td>\n",
       "      <td>United States</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>AustenMu</td>\n",
       "      <td>367</td>\n",
       "      <td>10754</td>\n",
       "      <td>Mom, wife, NICU RN, rescue kitten foster. I wr...</td>\n",
       "      <td>Springfield, MO</td>\n",
       "      <td>1265682481</td>\n",
       "      <td>@RolandMazzucch3 @curtquin @stormscomming @abr...</td>\n",
       "      <td>2022-09-20 20:35:03+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Missouri, USA</td>\n",
       "      <td>United States</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>andycollins10</td>\n",
       "      <td>1994</td>\n",
       "      <td>71074</td>\n",
       "      <td>Old hillbilly with lots of travels #resister B...</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>1264271250085613577</td>\n",
       "      <td>So the GOP went from “Covid is a hoax” to Covi...</td>\n",
       "      <td>2022-09-20 16:38:47+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Texas, USA</td>\n",
       "      <td>United States</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>AustenMu</td>\n",
       "      <td>367</td>\n",
       "      <td>10754</td>\n",
       "      <td>Mom, wife, NICU RN, rescue kitten foster. I wr...</td>\n",
       "      <td>Springfield, MO</td>\n",
       "      <td>1265682481</td>\n",
       "      <td>@RescueFelines @friskyfirefly @AdamKnott16 @Tr...</td>\n",
       "      <td>2022-09-20 13:14:34+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Missouri, USA</td>\n",
       "      <td>United States</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>ThunderRoo</td>\n",
       "      <td>2106</td>\n",
       "      <td>104461</td>\n",
       "      <td>Howdy there! 30yr old friendly red roo and pup...</td>\n",
       "      <td>Scottsdale, AZ</td>\n",
       "      <td>311931786</td>\n",
       "      <td>Like if you legitimately believe the 2020 elec...</td>\n",
       "      <td>2022-09-18 19:09:16+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Scottsdale, AZ</td>\n",
       "      <td>United States</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>dhenders11</td>\n",
       "      <td>895</td>\n",
       "      <td>7968</td>\n",
       "      <td>Undergrad major in philosophy, former world cl...</td>\n",
       "      <td>Marietta, GA</td>\n",
       "      <td>1257759875213516812</td>\n",
       "      <td>@EXANKANTHA @PapiTrumpo I will chalk that up t...</td>\n",
       "      <td>2022-09-18 01:32:05+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Fair Oaks, GA</td>\n",
       "      <td>United States</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username  author_followers  author_tweets  \\\n",
       "7       pe62050408               898          12215   \n",
       "33        AustenMu               367          10754   \n",
       "41        AustenMu               367          10754   \n",
       "110  andycollins10              1994          71074   \n",
       "152       AustenMu               367          10754   \n",
       "585     ThunderRoo              2106         104461   \n",
       "714     dhenders11               895           7968   \n",
       "\n",
       "                                    author_description  author_location  \\\n",
       "7                                                          Florida, USA   \n",
       "33   Mom, wife, NICU RN, rescue kitten foster. I wr...  Springfield, MO   \n",
       "41   Mom, wife, NICU RN, rescue kitten foster. I wr...  Springfield, MO   \n",
       "110  Old hillbilly with lots of travels #resister B...       Dallas, TX   \n",
       "152  Mom, wife, NICU RN, rescue kitten foster. I wr...  Springfield, MO   \n",
       "585  Howdy there! 30yr old friendly red roo and pup...   Scottsdale, AZ   \n",
       "714  Undergrad major in philosophy, former world cl...     Marietta, GA   \n",
       "\n",
       "               author_id                                               text  \\\n",
       "7    1456667468588535813  I thought Covid was a hoax? What’s with all th...   \n",
       "33            1265682481  @RolandMazzucch3 @curtquin @stormscomming @abr...   \n",
       "41            1265682481  @RolandMazzucch3 @curtquin @stormscomming @abr...   \n",
       "110  1264271250085613577  So the GOP went from “Covid is a hoax” to Covi...   \n",
       "152           1265682481  @RescueFelines @friskyfirefly @AdamKnott16 @Tr...   \n",
       "585            311931786  Like if you legitimately believe the 2020 elec...   \n",
       "714  1257759875213516812  @EXANKANTHA @PapiTrumpo I will chalk that up t...   \n",
       "\n",
       "                   created_at  retweets  replies  likes  quote_count  \\\n",
       "7   2022-09-20 23:13:24+00:00         0        0      2            0   \n",
       "33  2022-09-20 20:51:58+00:00         0        0      0            0   \n",
       "41  2022-09-20 20:35:03+00:00         0        1      0            0   \n",
       "110 2022-09-20 16:38:47+00:00         0        0      1            0   \n",
       "152 2022-09-20 13:14:34+00:00         0        1      0            0   \n",
       "585 2022-09-18 19:09:16+00:00         0        2      4            0   \n",
       "714 2022-09-18 01:32:05+00:00         0        0      0            0   \n",
       "\n",
       "     has_photo  has_video  tweet_location  tweet_country tweet_location_type  \n",
       "7        False      False  Wellington, FL  United States                city  \n",
       "33       False      False   Missouri, USA  United States               admin  \n",
       "41       False      False   Missouri, USA  United States               admin  \n",
       "110      False      False      Texas, USA  United States               admin  \n",
       "152      False      False   Missouri, USA  United States               admin  \n",
       "585      False      False  Scottsdale, AZ  United States                city  \n",
       "714      False      False   Fair Oaks, GA  United States                city  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.tweet_country == 'United States']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `requests`-based version\n",
    "\n",
    "If you want to do things without tweepy, here is some boilerplate code that should work. As you can see, it's much more complicated. Be grateful for the tweepy developers!! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import twitter_authentication as config\n",
    "import time\n",
    "\n",
    "# Save your bearer token in a file called twitter_authentication.py in this directory\n",
    "# Should look like this:\n",
    "# bearer_token = 'YOUR_BEARER_TOKEN_HERE'\n",
    "\n",
    "bearer_token = config.bearer_token\n",
    "query = '(#COVID) OR (#COVID-19)'\n",
    "out_file = 'raw_tweets.txt'\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "# Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "# expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
    "query_params = {'query': query,\n",
    "                'start_time': '2010-01-01T12:00:00Z',\n",
    "                'tweet.fields': 'author_id,public_metrics',\n",
    "                 'user.fields': 'username',\n",
    "                'expansions': 'author_id',\n",
    "                'max_results': 500\n",
    "               }\n",
    "\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    if next_token:\n",
    "        params['next_token'] = next_token\n",
    "    response = requests.request(\"GET\", search_url, headers=headers, params=params)\n",
    "    time.sleep(3.1)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_tweets(num_tweets, output_fh):\n",
    "    next_token = None\n",
    "    tweets_stored = 0\n",
    "    while tweets_stored < num_tweets:\n",
    "        headers = create_headers(bearer_token)\n",
    "        json_response = connect_to_endpoint(search_url, headers, query_params, next_token)\n",
    "        if json_response['meta']['result_count'] == 0:\n",
    "            break\n",
    "        author_dict = {x['id']: x['username'] for x in json_response['includes']['users']}\n",
    "        for tweet in json_response['data']:\n",
    "            try:\n",
    "                tweet['username'] = author_dict[tweet['author_id']]\n",
    "            except KeyError:\n",
    "                print(f\"No data for {tweet['author_id']}\")\n",
    "            output_fh.write(json.dumps(tweet) + '\\n')\n",
    "            tweets_stored += 1\n",
    "        try:\n",
    "            next_token = json_response['meta']['next_token']\n",
    "        except KeyError:\n",
    "            break\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(out_file, 'w') as f:\n",
    "        get_tweets(500, f)\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "with open(out_file, 'r') as f:\n",
    "    for row in f.readlines():\n",
    "        tweet = json.loads(row)\n",
    "        tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
